{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08282ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define PongGame class\n",
    "class PongGame:\n",
    "    def __init__(self):\n",
    "        self.WIDTH = 700\n",
    "        self.HEIGHT = 700\n",
    "        self.PADDLE_WIDTH = 20\n",
    "        self.PADDLE_HEIGHT = 100\n",
    "        self.BALL_RADIUS = 7\n",
    "        self.WINNING_SCORE = 20\n",
    "        self.FPS = 60\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.BLACK = (0, 0, 0)\n",
    "        self.left_score = 0\n",
    "        self.right_score = 0\n",
    "        self.left_vel = 10\n",
    "        self.right_vel = 20\n",
    "        self.reward = 0\n",
    "\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.WIDTH, self.HEIGHT))\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "        self.left_paddle = self.create_paddle(10, self.HEIGHT // 2 - self.PADDLE_HEIGHT // 2, self.left_vel)\n",
    "        self.right_paddle = self.create_paddle(self.WIDTH-30, self.HEIGHT // 2 - self.PADDLE_HEIGHT // 2, self.right_vel)\n",
    "        self.ball = self.create_ball(self.WIDTH // 2, self.HEIGHT // 2)\n",
    "\n",
    "    def create_paddle(self, x, y, vel):\n",
    "        return {\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"width\": self.PADDLE_WIDTH,\n",
    "            \"height\": self.PADDLE_HEIGHT,\n",
    "            \"vel\": vel,\n",
    "            \"direction\" : 0,\n",
    "        }\n",
    "\n",
    "    def create_ball(self, x, y):\n",
    "        return {\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"radius\": self.BALL_RADIUS,\n",
    "            \"x_vel\": 8,\n",
    "            \"y_vel\": 0,\n",
    "        }\n",
    "\n",
    "    def draw(self, render = False):\n",
    "        self.screen.fill(self.BLACK)\n",
    "\n",
    "        # Draw paddles\n",
    "        pygame.draw.rect(self.screen, self.WHITE, \n",
    "                         (self.left_paddle[\"x\"], self.left_paddle[\"y\"], \n",
    "                          self.left_paddle[\"width\"], self.left_paddle[\"height\"]))\n",
    "        pygame.draw.rect(self.screen, self.WHITE, \n",
    "                         (self.right_paddle[\"x\"], self.right_paddle[\"y\"], \n",
    "                          self.right_paddle[\"width\"], self.right_paddle[\"height\"]))\n",
    "\n",
    "        # Draw ball\n",
    "        pygame.draw.circle(self.screen, self.WHITE, \n",
    "                           (int(self.ball[\"x\"]), int(self.ball[\"y\"])), self.ball[\"radius\"])\n",
    "\n",
    "        # Draw middle separating line\n",
    "        for i in range(10, self.HEIGHT, self.HEIGHT // 20):\n",
    "            if i % 2 == 1:\n",
    "                continue\n",
    "            pygame.draw.rect(self.screen, self.WHITE, \n",
    "                             (self.WIDTH // 2 - 5, i, 10, self.HEIGHT // 20))\n",
    "\n",
    "        # Draw scores\n",
    "        score_font = pygame.font.SysFont(\"comicsans\", 50)\n",
    "        left_score_text = score_font.render(f\"{self.left_score}\", 1, self.WHITE)\n",
    "        right_score_text = score_font.render(f\"{self.right_score}\", 1, self.WHITE)\n",
    "        self.screen.blit(left_score_text, (self.WIDTH // 4 - left_score_text.get_width() // 2, 20))\n",
    "        self.screen.blit(right_score_text, \n",
    "                         (self.WIDTH * 3 // 4 - right_score_text.get_width() // 2, 20))\n",
    "        if render:\n",
    "            pygame.display.update()  # Refresh the screen\n",
    "\n",
    "    def move_ball(self):\n",
    "        self.ball[\"x\"] += self.ball[\"x_vel\"]\n",
    "        self.ball[\"y\"] += self.ball[\"y_vel\"]\n",
    "\n",
    "        # Ball collisions with top and bottom walls\n",
    "        if self.ball[\"y\"] <= 0 or self.ball[\"y\"] >= self.HEIGHT:\n",
    "            self.ball[\"y_vel\"] *= -1\n",
    "\n",
    "        # Ball collision with the left paddle\n",
    "        if (self.ball[\"y\"] >= self.left_paddle[\"y\"] and\n",
    "            self.ball[\"y\"] <= self.left_paddle[\"y\"] + self.left_paddle[\"height\"]):\n",
    "            if self.ball[\"x\"] - self.ball[\"radius\"] <= self.left_paddle[\"x\"] + self.left_paddle[\"width\"]:\n",
    "                self.ball[\"x\"] = self.left_paddle[\"x\"] + self.left_paddle[\"width\"] + self.ball[\"radius\"]\n",
    "                self.ball[\"x_vel\"] *= -1\n",
    "                \n",
    "                if self.ball[\"y_vel\"] < 0:\n",
    "                    ball_direction = 1\n",
    "                elif self.ball[\"y_vel\"] >0:\n",
    "                    ball_direction = -1\n",
    "                else:\n",
    "                    ball_direction = 0\n",
    "                middle_y = self.left_paddle[\"y\"] + self.left_paddle[\"height\"] / 2\n",
    "                difference_in_y = middle_y - self.ball[\"y\"]\n",
    "                reduction_factor = (self.left_paddle[\"height\"] / 2) / self.ball[\"x_vel\"]\n",
    "                red = difference_in_y / reduction_factor\n",
    "                if self.left_paddle[\"direction\"] != 0:\n",
    "                    self.ball[\"y_vel\"] += -1 * ball_direction * self.left_paddle[\"direction\"] * red\n",
    "                else:\n",
    "                    self.ball[\"y_vel\"] = -1 * red\n",
    "                \n",
    "\n",
    "        # Ball collision with the right paddle\n",
    "        if (self.ball[\"y\"] >= self.right_paddle[\"y\"] and\n",
    "            self.ball[\"y\"] <= self.right_paddle[\"y\"] + self.right_paddle[\"height\"]):\n",
    "            if self.ball[\"x\"] + self.ball[\"radius\"] >= self.right_paddle[\"x\"]:\n",
    "                self.ball[\"x\"] = self.right_paddle[\"x\"] - self.ball[\"radius\"]\n",
    "                self.ball[\"x_vel\"] *= -1\n",
    "                \n",
    "                if self.ball[\"y_vel\"] < 0:\n",
    "                    ball_direction = 1\n",
    "                elif self.ball[\"y_vel\"] >0:\n",
    "                    ball_direction = -1\n",
    "                else:\n",
    "                    ball_direction = 0\n",
    "                middle_y = self.right_paddle[\"y\"] + self.right_paddle[\"height\"] / 2\n",
    "                difference_in_y = middle_y - self.ball[\"y\"]\n",
    "                reduction_factor = (self.right_paddle[\"height\"] / 2) / self.ball[\"x_vel\"]\n",
    "                red = difference_in_y / reduction_factor\n",
    "                if self.right_paddle[\"direction\"] != 0:\n",
    "                    self.ball[\"y_vel\"] += -1 * ball_direction * self.right_paddle[\"direction\"] * red\n",
    "                else:\n",
    "                    self.ball[\"y_vel\"] = -1 * red\n",
    "                self.reward = 5\n",
    "    def track_ball_with_left_paddle(self):\n",
    "        # If the ball is above the paddle, move up\n",
    "        if self.ball[\"y\"] < self.left_paddle[\"y\"]:\n",
    "            self.left_paddle[\"y\"] -= self.left_paddle[\"vel\"]\n",
    "            self.left_paddle[\"direction\"] = 1\n",
    "        # If the ball is below the paddle, move down\n",
    "        elif self.ball[\"y\"] > self.left_paddle[\"y\"] + self.left_paddle[\"height\"]:\n",
    "            self.left_paddle[\"y\"] += self.left_paddle[\"vel\"]\n",
    "            self.left_paddle[\"direction\"] = -1\n",
    "        else:\n",
    "            self.left_paddle[\"direction\"] = 0\n",
    "    \n",
    "    def step(self, action, render = False):\n",
    "        # Action 0: move right paddle up, 1: move right paddle down\n",
    "        if action == 0 and self.right_paddle[\"y\"] > 0:\n",
    "            self.right_paddle[\"y\"] -= self.right_paddle[\"vel\"]\n",
    "            self.right_paddle[\"direction\"] = 1\n",
    "        elif action == 1 and self.right_paddle[\"y\"] < self.HEIGHT - self.PADDLE_HEIGHT:\n",
    "            self.right_paddle[\"y\"] += self.right_paddle[\"vel\"]\n",
    "            self.right_paddle[\"direction\"] = -1\n",
    "        else:\n",
    "            self.right_paddle[\"direction\"] = 0\n",
    "\n",
    "        self.reward = 0\n",
    "        self.move_ball()  # Move the ball\n",
    "\n",
    "        # Check if the ball missed the paddles\n",
    "        \n",
    "        done = False\n",
    "        if self.ball[\"x\"] < 0:  # Left side\n",
    "            self.right_score += 1\n",
    "            self.reset_ball()  # Reset the ball\n",
    "            self.reward = 10  # Positive reward when the ball goes left\n",
    "            done = self.right_score >= self.WINNING_SCORE\n",
    "        elif self.ball[\"x\"] > self.WIDTH:  # Right side\n",
    "            self.left_score += 1\n",
    "            self.reset_ball()  # Reset the ball\n",
    "            self.reward = -10\n",
    "            done = self.left_score >= self.WINNING_SCORE\n",
    "\n",
    "        self.draw(render)  # Draw the game\n",
    "        current_frame = pygame.surfarray.array3d(self.screen)  # Get the frame\n",
    "        current_frame = np.transpose(current_frame, (1, 0, 2))  # Adjust the frame shape\n",
    "        \n",
    "        return current_frame, self.reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.left_paddle = self.create_paddle(10, self.HEIGHT // 2 - self.PADDLE_HEIGHT // 2, self.left_vel)\n",
    "        self.right_paddle = self.create_paddle(self.WIDTH - 30, self.HEIGHT // 2 - self.PADDLE_HEIGHT // 2, self.right_vel)\n",
    "        self.reset_ball()\n",
    "        self.left_score = 0\n",
    "        self.right_score = 0\n",
    "        return self.get_frame()  # Return the initial frame\n",
    "\n",
    "    def reset_ball(self):\n",
    "        self.ball = self.create_ball(self.WIDTH // 2, self.HEIGHT // 2)\n",
    "        self.ball[\"x_vel\"] = self.ball[\"x_vel\"] if random.choice([True, False]) else (-1 * self.ball[\"x_vel\"])\n",
    "        self.ball[\"y_vel\"] = 0\n",
    "        angle = random.uniform(-45, 45)  # Random launch angle\n",
    "        self.ball[\"y_vel\"] = self.ball[\"x_vel\"] * np.tan(np.radians(angle))  # Calculate y-velocity\n",
    "\n",
    "    def get_frame(self):\n",
    "        self.draw()  # Draw the game\n",
    "        current_frame = pygame.surfarray.array3d(self.screen)  # Get the frame\n",
    "        current_frame = np.transpose(current_frame, (1, 0, 2))  # Adjust the frame shape\n",
    "        return current_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "992853a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import cv2\n",
    "import collections\n",
    "import torch as T\n",
    "import random\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Constants for Pong game\n",
    "WIDTH, HEIGHT = 700, 700\n",
    "FPS = 60\n",
    "PADDLE_WIDTH, PADDLE_HEIGHT = 20, 100\n",
    "BALL_RADIUS = 7\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "WINNING_SCORE = 10\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Function to preprocess Pygame frame for DQN\n",
    "def preprocess_frame(frame):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  # Grayscale\n",
    "    resized_frame = cv2.resize(gray_frame, (84, 84), interpolation=cv2.INTER_AREA)  # Resize\n",
    "    normalized_frame = resized_frame / 255.0  # Normalize\n",
    "    return np.expand_dims(normalized_frame, axis=0)  # For CNN\n",
    "\n",
    "# Frame stacker to stack consecutive frames\n",
    "class FrameStacker:\n",
    "    def __init__(self, stack_size):\n",
    "        self.stack_size = stack_size\n",
    "        self.stack = collections.deque(maxlen=stack_size)\n",
    "\n",
    "    def reset(self, initial_frame):\n",
    "        initial_frame = initial_frame.squeeze() \n",
    "        self.stack = collections.deque([initial_frame] * self.stack_size, maxlen=self.stack_size)\n",
    "        return np.array(self.stack)\n",
    "\n",
    "    def add_frame(self, frame):\n",
    "        frame = frame.squeeze()\n",
    "        self.stack.append(frame)\n",
    "        return np.array(self.stack)\n",
    "\n",
    "# Replay buffer to store transitions\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_transitions(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        indices = np.random.choice(max_mem, batch_size, replace=False)\n",
    "        return (\n",
    "            self.state_memory[indices],\n",
    "            self.action_memory[indices],\n",
    "            self.reward_memory[indices],\n",
    "            self.new_state_memory[indices],\n",
    "            self.terminal_memory[indices],\n",
    "        )\n",
    "\n",
    "# Deep Q-Network (DQN) definition\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, n_actions, input_dims, name, chkpt_dir):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
    "        self.conv1 = nn.Conv2d(input_dims[0], 32, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
    "        fc_input_dims = self.calculate_conv_output_dims(input_dims)\n",
    "        self.fc1 = nn.Linear(fc_input_dims, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def calculate_conv_output_dims(self, input_dims):\n",
    "        state = T.zeros(1, *input_dims)\n",
    "        dims = self.conv1(state)\n",
    "        dims = self.conv2(dims)\n",
    "        dims = self.conv3(dims)\n",
    "        return int(np.prod(dims.size()))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.conv1(state))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        actions = self.fc2(x)\n",
    "        return actions\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file, map_location=T.device(self.device)))\n",
    "\n",
    "# DQN agent to interact with the game\n",
    "class DQNAgent:\n",
    "    def __init__(self, gamma, epsilon, lr, n_actions, input_dims, mem_size, batch_size, eps_min=0.01, eps_dec=5e-7, name = None, chkpt_dir = None):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.n_actions = n_actions\n",
    "        self.input_dims = input_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_dec = eps_dec\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions)\n",
    "        self.q_eval = DeepQNetwork(lr, n_actions, input_dims, name+'_q_eval', chkpt_dir)\n",
    "        self.q_next = DeepQNetwork(lr, n_actions, input_dims, name+'_q_next', chkpt_dir)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            observation_np = np.array(observation) \n",
    "            state = T.tensor([observation_np], dtype=T.float).to(self.q_eval.device)\n",
    "            actions = self.q_eval(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice([0, 1])  # 0: up, 1: down\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "        \n",
    "    def save_models(self):\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "        self.replace_target_network()\n",
    "\n",
    "        states, actions, rewards, states_, dones = self.memory.sample_transitions(self.batch_size)\n",
    "        states = T.tensor(states, dtype=T.float).to(self.q_eval.device)  \n",
    "        actions = T.tensor(actions, dtype=T.long).to(self.q_eval.device)  \n",
    "        rewards = T.tensor(rewards, dtype=T.float).to(self.q_eval.device)\n",
    "        states_ = T.tensor(states_, dtype=T.float).to(self.q_eval.device)\n",
    "        dones = T.tensor(dones, dtype=T.bool).to(self.q_eval.device)\n",
    "        indices = np.arange(self.batch_size)\n",
    "\n",
    "        q_pred = self.q_eval(states)[indices, actions]\n",
    "        q_next = self.q_next(states_).max(dim=1)[0]\n",
    "\n",
    "        q_next[dones] = 0.0  # No future value if the game is over\n",
    "        q_target = rewards + self.gamma * q_next\n",
    "\n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
    "        loss.backward()\n",
    "        self.q_eval.optimizer.step()\n",
    "\n",
    "        self.learn_step_counter += 1\n",
    "        self.epsilon = max(self.epsilon - self.eps_dec, self.eps_min)\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        if self.learn_step_counter % 1000 == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ef4f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pong_game = PongGame()\n",
    "agent = DQNAgent(\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    lr=0.0001,\n",
    "    n_actions=2,  # Two actions: move up or down\n",
    "    input_dims=(4, 84, 84),  # Stacking four frames\n",
    "    mem_size=20000,\n",
    "    batch_size=32,\n",
    "    eps_min=0.1,\n",
    "    eps_dec=25e-7, \n",
    "    chkpt_dir='models/', \n",
    "    name='DQNAgent_final'\n",
    ")\n",
    "\n",
    "best_score = -np.inf\n",
    "scores =[]\n",
    "frame_stacker = FrameStacker(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8e781078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 87, Reward: 450, Score: 20, Average score: 10.363636363636363 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 88, Reward: 465, Score: 20, Average score: 10.47191011235955 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 89, Reward: 460, Score: 20, Average score: 10.577777777777778 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 90, Reward: 460, Score: 20, Average score: 10.68131868131868 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 91, Reward: 405, Score: 20, Average score: 10.782608695652174 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 92, Reward: 435, Score: 20, Average score: 10.881720430107526 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 93, Reward: 515, Score: 20, Average score: 10.97872340425532 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 94, Reward: 520, Score: 20, Average score: 11.073684210526316 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 95, Reward: 460, Score: 20, Average score: 11.166666666666666 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 96, Reward: 535, Score: 20, Average score: 11.257731958762887 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 97, Reward: 475, Score: 20, Average score: 11.346938775510203 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 98, Reward: 420, Score: 20, Average score: 11.434343434343434 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 99, Reward: 385, Score: 20, Average score: 11.52 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 100, Reward: 490, Score: 20, Average score: 11.71 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 101, Reward: 355, Score: 20, Average score: 11.91 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 102, Reward: 490, Score: 20, Average score: 12.1 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 103, Reward: 515, Score: 20, Average score: 12.3 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 104, Reward: 505, Score: 20, Average score: 12.49 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 105, Reward: 515, Score: 20, Average score: 12.69 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 106, Reward: 465, Score: 20, Average score: 12.88 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 107, Reward: 510, Score: 20, Average score: 13.06 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 108, Reward: 420, Score: 20, Average score: 13.23 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 109, Reward: 460, Score: 20, Average score: 13.39 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 110, Reward: 430, Score: 20, Average score: 13.58 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 111, Reward: 475, Score: 20, Average score: 13.76 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 112, Reward: 380, Score: 20, Average score: 13.95 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 113, Reward: 435, Score: 20, Average score: 14.13 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 114, Reward: 420, Score: 20, Average score: 14.29 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 115, Reward: 560, Score: 20, Average score: 14.46 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 116, Reward: 485, Score: 20, Average score: 14.61 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 117, Reward: 435, Score: 20, Average score: 14.77 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 118, Reward: 425, Score: 20, Average score: 14.95 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 119, Reward: 410, Score: 20, Average score: 15.11 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 120, Reward: 500, Score: 20, Average score: 15.29 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 121, Reward: 485, Score: 20, Average score: 15.45 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 122, Reward: 480, Score: 20, Average score: 15.62 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 123, Reward: 430, Score: 20, Average score: 15.78 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 124, Reward: 450, Score: 20, Average score: 15.97 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 125, Reward: 360, Score: 20, Average score: 16.14 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 126, Reward: 335, Score: 20, Average score: 16.33 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 127, Reward: 465, Score: 20, Average score: 16.52 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 128, Reward: 460, Score: 20, Average score: 16.66 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 129, Reward: 400, Score: 20, Average score: 16.83 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 130, Reward: 420, Score: 20, Average score: 16.98 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 131, Reward: 480, Score: 20, Average score: 17.13 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 132, Reward: 505, Score: 20, Average score: 17.31 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 133, Reward: 530, Score: 20, Average score: 17.48 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 134, Reward: 500, Score: 20, Average score: 17.63 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 135, Reward: 480, Score: 20, Average score: 17.78 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 136, Reward: 445, Score: 20, Average score: 17.94 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 137, Reward: 455, Score: 20, Average score: 18.11 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 138, Reward: 440, Score: 20, Average score: 18.29 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 139, Reward: 460, Score: 20, Average score: 18.45 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 140, Reward: 420, Score: 20, Average score: 18.59 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 141, Reward: 405, Score: 20, Average score: 18.76 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 142, Reward: 475, Score: 20, Average score: 18.86 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 143, Reward: 475, Score: 20, Average score: 19.02 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 144, Reward: 475, Score: 20, Average score: 19.13 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 145, Reward: 520, Score: 20, Average score: 19.24 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 146, Reward: 450, Score: 20, Average score: 19.3 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 147, Reward: 440, Score: 20, Average score: 19.43 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 148, Reward: 440, Score: 20, Average score: 19.47 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 149, Reward: 435, Score: 20, Average score: 19.58 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 150, Reward: 510, Score: 20, Average score: 19.6 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 151, Reward: 480, Score: 20, Average score: 19.65 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 152, Reward: 415, Score: 20, Average score: 19.65 , Epsilon: 0.1\n",
      "Episode 153, Reward: 415, Score: 20, Average score: 19.65 , Epsilon: 0.1\n",
      "Episode 154, Reward: 385, Score: 20, Average score: 19.75 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 155, Reward: 450, Score: 20, Average score: 19.81 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 156, Reward: 410, Score: 20, Average score: 19.85 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 157, Reward: 480, Score: 20, Average score: 19.85 , Epsilon: 0.1\n",
      "Episode 158, Reward: 515, Score: 20, Average score: 19.85 , Epsilon: 0.1\n",
      "Episode 159, Reward: 440, Score: 20, Average score: 19.88 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 160, Reward: 460, Score: 20, Average score: 19.92 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 161, Reward: 405, Score: 20, Average score: 19.92 , Epsilon: 0.1\n",
      "Episode 162, Reward: 425, Score: 20, Average score: 19.92 , Epsilon: 0.1\n",
      "Episode 163, Reward: 475, Score: 20, Average score: 19.92 , Epsilon: 0.1\n",
      "Episode 164, Reward: 560, Score: 20, Average score: 20.0 , Epsilon: 0.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Episode 165, Reward: 440, Score: 20, Average score: 20.0 , Epsilon: 0.1\n",
      "Episode 166, Reward: 435, Score: 20, Average score: 20.0 , Epsilon: 0.1\n",
      "Episode 167, Reward: 460, Score: 20, Average score: 20.0 , Epsilon: 0.1\n",
      "Episode 168, Reward: 485, Score: 20, Average score: 20.0 , Epsilon: 0.1\n",
      "Episode 169, Reward: 495, Score: 20, Average score: 20.0 , Epsilon: 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m stacked_frames_ \u001b[38;5;241m=\u001b[39m frame_stacker\u001b[38;5;241m.\u001b[39madd_frame(next_frame)  \u001b[38;5;66;03m# Stack frames\u001b[39;00m\n\u001b[1;32m     15\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_transition(stacked_frames, action, reward, stacked_frames_, done)  \u001b[38;5;66;03m# Store transitions\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m agent\u001b[38;5;241m.\u001b[39mlearn()  \u001b[38;5;66;03m# Train the DQN agent on stored transitions\u001b[39;00m\n\u001b[1;32m     18\u001b[0m stacked_frames \u001b[38;5;241m=\u001b[39m stacked_frames_  \u001b[38;5;66;03m# Update current state\u001b[39;00m\n\u001b[1;32m     19\u001b[0m ep_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# Add to the score\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 179\u001b[0m, in \u001b[0;36mDQNAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m q_target \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m q_next\n\u001b[1;32m    178\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_eval\u001b[38;5;241m.\u001b[39mloss(q_target, q_pred)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_eval\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 179\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_eval\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn_step_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "n_episodes = 200\n",
    "for episode in range(87,200):\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    frame = preprocess_frame(pong_game.reset())  # Preprocess the first frame\n",
    "    stacked_frames = frame_stacker.reset(frame)  # Initialize frame stack\n",
    "\n",
    "    while not done:\n",
    "        pong_game.track_ball_with_left_paddle()\n",
    "        action = agent.choose_action(stacked_frames)  # Get the action\n",
    "        next_frame, reward, done = pong_game.step(action)  # Get next state and reward\n",
    "        next_frame = preprocess_frame(next_frame)  # Preprocess the next frame\n",
    "        stacked_frames_ = frame_stacker.add_frame(next_frame)  # Stack frames\n",
    "\n",
    "        agent.store_transition(stacked_frames, action, reward, stacked_frames_, done)  # Store transitions\n",
    "        agent.learn()  # Train the DQN agent on stored transitions\n",
    "\n",
    "        stacked_frames = stacked_frames_  # Update current state\n",
    "        ep_reward += reward  # Add to the score\n",
    "        \n",
    "    scores.append(pong_game.right_score)\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print(f\"Episode {episode}, Reward: {ep_reward}, Score: {pong_game.right_score}, Average score: {avg_score} , Epsilon: {agent.epsilon}\")  # Display the episode score\n",
    "    if avg_score > best_score:\n",
    "        agent.save_models()\n",
    "        best_score = avg_score\n",
    "\n",
    "pygame.quit()  # Clean up Pygame resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a62b4828-d380-432d-a8c4-b53fc56e5a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepQNetwork(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (loss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.q_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617554d-3dcc-4ce2-9e0a-69844a566b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f25bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4b679b14-d6e1-4e7f-9034-8774ac8961b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n"
     ]
    }
   ],
   "source": [
    "agent.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e39a5a15-be96-4206-9ecf-5243fce486cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2 = DQNAgent(\n",
    "    gamma=0.99,\n",
    "    epsilon=0,\n",
    "    lr=0.0001,\n",
    "    n_actions=2,  # Two actions: move up or down\n",
    "    input_dims=(4, 84, 84),  # Stacking four frames\n",
    "    mem_size=20000,\n",
    "    batch_size=32,\n",
    "    eps_min=0.1,\n",
    "    eps_dec=1e-4, \n",
    "    chkpt_dir='models/', \n",
    "    name='DQNAgent_final'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cc21756-8925-4688-a2df-8d56167da7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n"
     ]
    }
   ],
   "source": [
    "agent2.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a09feec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients(input_images, model, baseline, steps=50, action=None):\n",
    "    \n",
    "    # Convert inputs to PyT tensors\n",
    "    input_images = T.tensor(input_images, dtype=T.float32).unsqueeze(0).to(model.device)\n",
    "    baseline = T.tensor(baseline, dtype=T.float32).unsqueeze(0).to(model.device)\n",
    "    \n",
    "    # Activate evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Generate the scaled images\n",
    "    scaled_images = [baseline + (float(i) / steps) * (input_images - baseline) for i in range(steps + 1)]\n",
    "    scaled_images = T.cat(scaled_images, dim=0)\n",
    "\n",
    "    # Enable gradient calculation on scaled images\n",
    "    scaled_images.requires_grad_(True)\n",
    "\n",
    "    # Compute the model output\n",
    "    outputs = model(scaled_images)\n",
    "\n",
    "    # Focus on the outputs for the specified action\n",
    "    if action is not None:\n",
    "        outputs = outputs[:, action]\n",
    "    grads = T.autograd.grad(outputs, scaled_images, grad_outputs=T.ones_like(outputs), create_graph=True)[0]\n",
    "\n",
    "    # Average the gradients across the steps\n",
    "    avg_grads = T.mean(grads, dim=0)\n",
    "\n",
    "    # Compute the integrated gradients\n",
    "    integrated_grads = (input_images - baseline) * avg_grads\n",
    "    integrated_grads = integrated_grads.detach().cpu().numpy().squeeze()\n",
    "    \n",
    "    return integrated_grads[-1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28875568-71ec-459c-9499-4da3d303a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pong_game = PongGame()\n",
    "frame_stacker = FrameStacker(4)\n",
    "done = False\n",
    "score = 0\n",
    "frame = preprocess_frame(pong_game.reset())  \n",
    "stacked_frames = frame_stacker.reset(frame) \n",
    "\n",
    "while not done:\n",
    "#     pong_game.clock.tick(60)\n",
    "    # pong_game.track_ball_with_left_paddle()\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    action = agent2.choose_action(stacked_frames)\n",
    "    next_frame, reward, done = pong_game.step(keys = keys, action = action, render = True)\n",
    "    score += reward\n",
    "    original_image = next_frame\n",
    "    next_frame = preprocess_frame(next_frame)  # Preprocess the next frame\n",
    "    stacked_frames = frame_stacker.add_frame(next_frame)\n",
    "\n",
    "    baseline = np.zeros_like(stacked_frames)\n",
    "    saliency_map = integrated_gradients(stacked_frames, agent2.q_eval, baseline, 50, action)\n",
    "    resized_saliency = cv2.resize(saliency_map, (700, 700), interpolation=cv2.INTER_LINEAR)\n",
    "    red_overlay = np.zeros_like(original_image)\n",
    "    red_overlay[resized_saliency > 0] = [0, 255, 0]\n",
    "    \n",
    "    blue, green, red = cv2.split(original_image)\n",
    "    original_image = cv2.merge((red, green, blue))\n",
    "\n",
    "    # Overlay red saliency map on the original frame\n",
    "    overlay = original_image.copy().astype(np.uint8)\n",
    "    overlay = cv2.addWeighted(overlay, 0.7, red_overlay.astype(np.uint8), 0.3, 0)\n",
    "\n",
    "#         cv2.imshow(\"Saliency map\", resized_saliency)\n",
    "\n",
    "    cv2.imshow(\"Overlayed Saliency map\", overlay)\n",
    "    cv2.waitKey(1)\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    won = False\n",
    "    if pong_game.left_score >= pong_game.WINNING_SCORE:\n",
    "        won = True\n",
    "        win_text = \"You won!!\"\n",
    "        t_color = \"BLUE\"\n",
    "    elif pong_game.right_score >= pong_game.WINNING_SCORE:\n",
    "        won = True\n",
    "        win_text = \"DQN won!!\"\n",
    "        t_color = \"RED\"\n",
    "\n",
    "    if won:\n",
    "        text = pong_game.text_font.render(win_text, 1, t_color)\n",
    "        pong_game.screen.blit(text, (WIDTH//2 - text.get_width() //\n",
    "                        2, HEIGHT//2 - text.get_height()//2))\n",
    "        pygame.display.update()\n",
    "        pygame.time.delay(5000)\n",
    "        pong_game.reset()\n",
    "            \n",
    "pygame.quit() \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
